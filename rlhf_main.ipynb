{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabri/Anaconda3/envs/sycamore/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# bridge env (Gabriel's)\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pickle\n",
    "from discrete_blocks import discrete_block as Block\n",
    "from relative_single_agent import SACSupervisorSparse,generous_reward,punitive_reward,modular_reward\n",
    "from discrete_simulator import DiscreteSimulator as Sim, Transition\n",
    "import discrete_graphics as gr\n",
    "\n",
    "# # rlhf\n",
    "# import random\n",
    "# from imitation.algorithms import preference_comparisons\n",
    "# from imitation.rewards.reward_nets import BasicRewardNet\n",
    "# from imitation.util.networks import RunningNorm\n",
    "# from imitation.util.util import make_vec_env\n",
    "# from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "# import gymnasium as gym\n",
    "# from stable_baselines3 import PPO\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"rlhf_main.ipynb\"\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "USE_WANDB = False\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"rlhf_main.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blocks\n",
    "hexagon = Block([[1,0,0],[1,1,1],[1,1,0],[0,2,1],[0,1,0],[0,1,1]],muc=0.5)\n",
    "linkr = Block([[0,0,0],[0,1,1],[1,0,0],[1,0,1],[1,1,1],[0,1,0]],muc=0.5) \n",
    "linkl = Block([[0,0,0],[0,1,1],[1,0,0],[0,1,0],[0,0,1],[-1,1,1]],muc=0.5) \n",
    "linkh = Block([[0,0,0],[0,1,1],[1,0,0],[-1,2,1],[0,1,0],[0,2,1]],muc=0.5)\n",
    "#target = Block([[0,0,1],[1,0,1]])\n",
    "target = Block([[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config = {'train_n_episodes':100,\n",
    "            'train_l_buffer':200,\n",
    "            'ep_batch_size':32,\n",
    "            'ep_use_mask':True,\n",
    "            'agent_discount_f':0.1, # 1-gamma\n",
    "            'agent_last_only':True,\n",
    "            'reward': 'modular',\n",
    "            'torch_device':'cpu',\n",
    "            'SEnc_n_channels':64,\n",
    "            'SEnc_n_internal_layer':2,\n",
    "            'SEnc_stride':1,\n",
    "            'SEnc_order_insensitive':True,\n",
    "            'SAC_n_fc_layer':3,\n",
    "            'SAC_n_neurons':128,\n",
    "            'SAC_batch_norm':True,\n",
    "            'Q_duel':True,\n",
    "            'opt_lr':1e-4,\n",
    "            'opt_pol_over_val': 1,\n",
    "            'opt_tau': 5e-4,\n",
    "            'opt_weight_decay':0.0001,\n",
    "            'opt_exploration_factor':0.001,\n",
    "            'agent_exp_strat':'softmax',\n",
    "            'agent_epsilon':0.05, # not needed in sac\n",
    "            'opt_max_norm': 2,\n",
    "            'opt_target_entropy':1.8,\n",
    "            'opt_value_clip':False,\n",
    "            'opt_entropy_penalty':False,\n",
    "            'opt_Q_reduction': 'min',\n",
    "            'V_optimistic':False,\n",
    "            'reward_failure':-1,\n",
    "            'reward_action':{'Ph': -0.2, 'L':-0.1},\n",
    "            'reward_closer':0.4,\n",
    "            'reward_nsides': 0.1,\n",
    "            'reward_success':1,\n",
    "            'reward_opposite_sides':0,\n",
    "            'opt_lower_bound_Vt':-2,\n",
    "            'gap_range':[2,6]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gym (env + agent)\n",
    "from single_agent_gym import ReplayDiscreteGymSupervisor\n",
    "\n",
    "gym = ReplayDiscreteGymSupervisor(config,\n",
    "              agent_type=SACSupervisorSparse,\n",
    "              use_wandb=USE_WANDB,\n",
    "              actions= ['Ph'], # place-hold only necessary action\n",
    "              block_type=[hexagon],\n",
    "              random_targets='random_gap', \n",
    "              targets_loc=[[2,0],[6,0]], \n",
    "              n_robots=2, \n",
    "              max_blocks = 10,\n",
    "              targets=[target]*2,\n",
    "              max_interfaces = 50,\n",
    "              log_freq = 5,\n",
    "              maxs = [9,6]) # grid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Reward Model\n",
    "from rlhf_reward_model import RewardLinear\n",
    "\n",
    "reward_model = RewardLinear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Fragmenter\n",
    "from rlhf_fragmenter import RandomFragmenter\n",
    "\n",
    "fragmenter = RandomFragmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Preference Gatherer (human/synthetic)\n",
    "from rlhf_preference_gatherer import SyntheticPreferenceGatherer\n",
    "\n",
    "gatherer = SyntheticPreferenceGatherer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Transitions' from 'typing' (/home/sabri/Anaconda3/envs/sycamore/lib/python3.9/typing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create Preference Model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrlhf_preference_model\u001b[39;00m \u001b[39mimport\u001b[39;00m PreferenceModel\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m preference_model \u001b[39m=\u001b[39m PreferenceModel(reward_model)\n",
      "File \u001b[0;32m~/Projects/SycamoreProject/mySycamore/rlhf_preference_model.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mth\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     Optional,\n\u001b[1;32m      5\u001b[0m     Sequence,\n\u001b[1;32m      6\u001b[0m     Tuple,\n\u001b[1;32m      7\u001b[0m     cast,\n\u001b[1;32m      8\u001b[0m     Transitions\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrlhf_reward_model\u001b[39;00m \u001b[39mimport\u001b[39;00m RewardModel, RewardLinear\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Transitions' from 'typing' (/home/sabri/Anaconda3/envs/sycamore/lib/python3.9/typing.py)"
     ]
    }
   ],
   "source": [
    "# Create Preference Model\n",
    "from rlhf_preference_model import PreferenceModel\n",
    "\n",
    "preference_model = PreferenceModel(reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Reward Trainer\n",
    "from rlhf_reward_trainer import LinearRewardTrainer\n",
    "\n",
    "reward_trainer = LinearRewardTrainer(preference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Preference Comparisons, the main interface\n",
    "from rlhf_preference_comparisons import PreferenceComparisons\n",
    "\n",
    "pref_comparisons = PreferenceComparisons(\n",
    "    gym,\n",
    "    reward_model,\n",
    "    num_iterations=5,  # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    initial_epoch_multiplier=4,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Original code\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rng \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mdefault_rng(\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m venv \u001b[39m=\u001b[39m make_vec_env(\u001b[39m\"\u001b[39m\u001b[39mPendulum-v1\u001b[39m\u001b[39m\"\u001b[39m, rng\u001b[39m=\u001b[39mrng)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m reward_net \u001b[39m=\u001b[39m BasicRewardNet(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     venv\u001b[39m.\u001b[39mobservation_space, venv\u001b[39m.\u001b[39maction_space, normalize_input_layer\u001b[39m=\u001b[39mRunningNorm\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sabri/Projects/SycamoreProject/mySycamore/rlhf_main.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Original code\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "venv = make_vec_env(\"Pendulum-v1\", rng=rng)\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(\n",
    "    warning_threshold=0,\n",
    "    rng=rng,\n",
    ")\n",
    "gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=3,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "\n",
    "# Several hyperparameters (reward_epochs, ppo_clip_range, ppo_ent_coef,\n",
    "# ppo_gae_lambda, ppo_n_epochs, discount_factor, use_sde, sde_sample_freq,\n",
    "# ppo_lr, exploration_frac, num_iterations, initial_comparison_frac,\n",
    "# initial_epoch_multiplier, query_schedule) used in this example have been\n",
    "# approximately fine-tuned to reach a reasonable level of performance.\n",
    "agent = PPO(\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=2048 // venv.num_envs,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=2e-3,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.05,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5,  # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=100,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    initial_epoch_multiplier=4,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reward Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [20, 51, 41, 34, 29, 25]\n",
      "Collecting 40 fragments (4000 transitions)\n",
      "Requested 3800 transitions but only 0 in buffer. Sampling 3800 additional transitions.\n",
      "Sampling 200 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 20 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 12/12 [00:00<00:00, 17.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_len_mean         | 200      |\n",
      "|    agent/rollout/ep_rew_mean         | -1.2e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 29.5     |\n",
      "|    agent/time/fps                    | 3775     |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 2048     |\n",
      "---------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    agent/rollout/ep_len_mean            | 200      |\n",
      "|    agent/rollout/ep_rew_mean            | -1.2e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | 29.5     |\n",
      "|    agent/time/fps                       | 3.78e+03 |\n",
      "|    agent/time/iterations                | 1        |\n",
      "|    agent/time/time_elapsed              | 0        |\n",
      "|    agent/time/total_timesteps           | 2.05e+03 |\n",
      "|    agent/train/approx_kl                | 0.0012   |\n",
      "|    agent/train/clip_fraction            | 0.0603   |\n",
      "|    agent/train/clip_range               | 0.1      |\n",
      "|    agent/train/entropy_loss             | -1.43    |\n",
      "|    agent/train/explained_variance       | -0.284   |\n",
      "|    agent/train/learning_rate            | 0.002    |\n",
      "|    agent/train/loss                     | 0.0554   |\n",
      "|    agent/train/n_updates                | 10       |\n",
      "|    agent/train/policy_gradient_loss     | -0.00143 |\n",
      "|    agent/train/std                      | 1.01     |\n",
      "|    agent/train/value_loss               | 0.615    |\n",
      "|    preferences/entropy                  | 0.0307   |\n",
      "|    reward/epoch-0/train/accuracy        | 0.15     |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.0639   |\n",
      "|    reward/epoch-0/train/loss            | 1.55     |\n",
      "|    reward/epoch-1/train/accuracy        | 0.45     |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.0639   |\n",
      "|    reward/epoch-1/train/loss            | 1.26     |\n",
      "|    reward/epoch-10/train/accuracy       | 0.9      |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.0639   |\n",
      "|    reward/epoch-10/train/loss           | 0.248    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.9      |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.0639   |\n",
      "|    reward/epoch-11/train/loss           | 0.241    |\n",
      "|    reward/epoch-2/train/accuracy        | 0.6      |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.0639   |\n",
      "|    reward/epoch-2/train/loss            | 0.865    |\n",
      "|    reward/epoch-3/train/accuracy        | 0.7      |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.0639   |\n",
      "|    reward/epoch-3/train/loss            | 0.597    |\n",
      "|    reward/epoch-4/train/accuracy        | 0.9      |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.0639   |\n",
      "|    reward/epoch-4/train/loss            | 0.437    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.9      |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.0639   |\n",
      "|    reward/epoch-5/train/loss            | 0.354    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.9      |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.0639   |\n",
      "|    reward/epoch-6/train/loss            | 0.31     |\n",
      "|    reward/epoch-7/train/accuracy        | 0.9      |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.0639   |\n",
      "|    reward/epoch-7/train/loss            | 0.286    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.9      |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.0639   |\n",
      "|    reward/epoch-8/train/loss            | 0.269    |\n",
      "|    reward/epoch-9/train/accuracy        | 0.9      |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.0639   |\n",
      "|    reward/epoch-9/train/loss            | 0.257    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.9      |\n",
      "|    final/train/gt_reward_loss           | 0.0639   |\n",
      "|    final/train/loss                     | 0.241    |\n",
      "------------------------------------------------------\n",
      "Collecting 102 fragments (10200 transitions)\n",
      "Requested 9690 transitions but only 1600 in buffer. Sampling 8090 additional transitions.\n",
      "Sampling 510 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 71 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:00<00:00,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.14e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 33.5         |\n",
      "|    agent/time/fps                    | 2438         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0011954873 |\n",
      "|    agent/train/clip_fraction         | 0.0603       |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.43        |\n",
      "|    agent/train/explained_variance    | -0.284       |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0554       |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00143     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.615        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.14e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 33.5      |\n",
      "|    agent/time/fps                      | 2.44e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 4.1e+03   |\n",
      "|    agent/train/approx_kl               | 0.00187   |\n",
      "|    agent/train/clip_fraction           | 0.0724    |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.43     |\n",
      "|    agent/train/explained_variance      | 0.67      |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.037     |\n",
      "|    agent/train/n_updates               | 20        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00302  |\n",
      "|    agent/train/std                     | 1.01      |\n",
      "|    agent/train/value_loss              | 0.219     |\n",
      "|    preferences/entropy                 | 7.34e-06  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.917     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0133    |\n",
      "|    reward/epoch-0/train/loss           | 0.223     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.917     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0133    |\n",
      "|    reward/epoch-1/train/loss           | 0.199     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.938     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0133    |\n",
      "|    reward/epoch-2/train/loss           | 0.188     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.938     |\n",
      "|    final/train/gt_reward_loss          | 0.0133    |\n",
      "|    final/train/loss                    | 0.188     |\n",
      "------------------------------------------------------\n",
      "Collecting 82 fragments (8200 transitions)\n",
      "Requested 7790 transitions but only 1600 in buffer. Sampling 6190 additional transitions.\n",
      "Sampling 410 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 112 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:00<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.17e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 35           |\n",
      "|    agent/time/fps                    | 3588         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0018702098 |\n",
      "|    agent/train/clip_fraction         | 0.0724       |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.43        |\n",
      "|    agent/train/explained_variance    | 0.67         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.037        |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00302     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.219        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.17e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 35        |\n",
      "|    agent/time/fps                      | 3.59e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 6.14e+03  |\n",
      "|    agent/train/approx_kl               | 0.00212   |\n",
      "|    agent/train/clip_fraction           | 0.115     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.43     |\n",
      "|    agent/train/explained_variance      | 0.882     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.00993   |\n",
      "|    agent/train/n_updates               | 30        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00494  |\n",
      "|    agent/train/std                     | 1         |\n",
      "|    agent/train/value_loss              | 0.0876    |\n",
      "|    preferences/entropy                 | 0.00253   |\n",
      "|    reward/epoch-0/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0102    |\n",
      "|    reward/epoch-0/train/loss           | 0.163     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.953     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0102    |\n",
      "|    reward/epoch-1/train/loss           | 0.142     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0102    |\n",
      "|    reward/epoch-2/train/loss           | 0.126     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.977     |\n",
      "|    final/train/gt_reward_loss          | 0.0102    |\n",
      "|    final/train/loss                    | 0.126     |\n",
      "------------------------------------------------------\n",
      "Collecting 68 fragments (6800 transitions)\n",
      "Requested 6460 transitions but only 1600 in buffer. Sampling 4860 additional transitions.\n",
      "Sampling 340 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 146 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:01<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.19e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 34.4         |\n",
      "|    agent/time/fps                    | 2599         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0021171272 |\n",
      "|    agent/train/clip_fraction         | 0.115        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.43        |\n",
      "|    agent/train/explained_variance    | 0.882        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00993      |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00494     |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.0876       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.19e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 34.4      |\n",
      "|    agent/time/fps                      | 2.6e+03   |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 8.19e+03  |\n",
      "|    agent/train/approx_kl               | 0.00255   |\n",
      "|    agent/train/clip_fraction           | 0.151     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.42     |\n",
      "|    agent/train/explained_variance      | 0.931     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.00995   |\n",
      "|    agent/train/n_updates               | 40        |\n",
      "|    agent/train/policy_gradient_loss    | -0.0081   |\n",
      "|    agent/train/std                     | 0.997     |\n",
      "|    agent/train/value_loss              | 0.0945    |\n",
      "|    preferences/entropy                 | 0.00952   |\n",
      "|    reward/epoch-0/train/accuracy       | 0.928     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.015     |\n",
      "|    reward/epoch-0/train/loss           | 0.161     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.918     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00938   |\n",
      "|    reward/epoch-1/train/loss           | 0.176     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.944     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00878   |\n",
      "|    reward/epoch-2/train/loss           | 0.142     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.944     |\n",
      "|    final/train/gt_reward_loss          | 0.00878   |\n",
      "|    final/train/loss                    | 0.142     |\n",
      "------------------------------------------------------\n",
      "Collecting 58 fragments (5800 transitions)\n",
      "Requested 5510 transitions but only 1600 in buffer. Sampling 3910 additional transitions.\n",
      "Sampling 290 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 175 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:01<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.21e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 33.6         |\n",
      "|    agent/time/fps                    | 3353         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0025484716 |\n",
      "|    agent/train/clip_fraction         | 0.151        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.42        |\n",
      "|    agent/train/explained_variance    | 0.931        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00995      |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.0081      |\n",
      "|    agent/train/std                   | 0.997        |\n",
      "|    agent/train/value_loss            | 0.0945       |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.21e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 33.6      |\n",
      "|    agent/time/fps                      | 3.35e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 1.02e+04  |\n",
      "|    agent/train/approx_kl               | 0.00395   |\n",
      "|    agent/train/clip_fraction           | 0.208     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.43     |\n",
      "|    agent/train/explained_variance      | 0.94      |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.00143   |\n",
      "|    agent/train/n_updates               | 50        |\n",
      "|    agent/train/policy_gradient_loss    | -0.0119   |\n",
      "|    agent/train/std                     | 1.01      |\n",
      "|    agent/train/value_loss              | 0.101     |\n",
      "|    preferences/entropy                 | 0.0226    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.943     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0128    |\n",
      "|    reward/epoch-0/train/loss           | 0.143     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0128    |\n",
      "|    reward/epoch-1/train/loss           | 0.138     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.943     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0129    |\n",
      "|    reward/epoch-2/train/loss           | 0.132     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.943     |\n",
      "|    final/train/gt_reward_loss          | 0.0129    |\n",
      "|    final/train/loss                    | 0.132     |\n",
      "------------------------------------------------------\n",
      "Collecting 50 fragments (5000 transitions)\n",
      "Requested 4750 transitions but only 1600 in buffer. Sampling 3150 additional transitions.\n",
      "Sampling 250 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 200 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 200        |\n",
      "|    agent/rollout/ep_rew_mean         | -1.2e+03   |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 32.3       |\n",
      "|    agent/time/fps                    | 4051       |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 12288      |\n",
      "|    agent/train/approx_kl             | 0.00394712 |\n",
      "|    agent/train/clip_fraction         | 0.208      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -1.43      |\n",
      "|    agent/train/explained_variance    | 0.94       |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.00143    |\n",
      "|    agent/train/n_updates             | 50         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0119    |\n",
      "|    agent/train/std                   | 1.01       |\n",
      "|    agent/train/value_loss            | 0.101      |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -1.2e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 32.3     |\n",
      "|    agent/time/fps                      | 4.05e+03 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.23e+04 |\n",
      "|    agent/train/approx_kl               | 0.00262  |\n",
      "|    agent/train/clip_fraction           | 0.11     |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.43    |\n",
      "|    agent/train/explained_variance      | 0.97     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.118    |\n",
      "|    agent/train/n_updates               | 60       |\n",
      "|    agent/train/policy_gradient_loss    | -0.004   |\n",
      "|    agent/train/std                     | 1.02     |\n",
      "|    agent/train/value_loss              | 0.0939   |\n",
      "|    preferences/entropy                 | 0.0155   |\n",
      "|    reward/epoch-0/train/accuracy       | 0.946    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0116   |\n",
      "|    reward/epoch-0/train/loss           | 0.149    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.942    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0116   |\n",
      "|    reward/epoch-1/train/loss           | 0.135    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.924    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0116   |\n",
      "|    reward/epoch-2/train/loss           | 0.152    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.924    |\n",
      "|    final/train/gt_reward_loss          | 0.0116   |\n",
      "|    final/train/loss                    | 0.152    |\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reward_loss': 0.1524055983339037, 'reward_accuracy': 0.9241071428571429}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=5_000,\n",
    "    total_comparisons=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Agent Training on Learned Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f59b0460f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PPO(\n",
    "    seed=0,\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=learned_reward_venv,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    n_epochs=10,\n",
    "    n_steps=2048 // learned_reward_venv.num_envs,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    learning_rate=2e-3,\n",
    ")\n",
    "learner.learn(1_000)  # Note: set to 100_000 to train a proficient expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -1339 +/- 117\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "n_eval_episodes = 10\n",
    "reward_mean, reward_std = evaluate_policy(learner.policy, venv, n_eval_episodes)\n",
    "reward_stderr = reward_std / np.sqrt(n_eval_episodes)\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycamore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
