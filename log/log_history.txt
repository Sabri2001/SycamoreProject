RLHF corrected (trained reward applied during agent training)

31/10 (gamma = 1 used for reward model): 
- two trainings locally with synthetic fb
- one training on jupyter with synthetic fb -> pb: far too few reward training episodes (forgot to change)

- agents: one trained with Gabriel fb, one with rlhf_local2 (episodes: 10 000) -> TO CHECK!!


07/11 (gamma reward model corrected, evaluation with greedy):
- Added loss tracking for agent training (synth_fb_1) -> weird loss for first few steps (int)
- Weird loss corrected (synth_fb_2)
- Success rate during agent training implemented (synth_fb_3)
- Reward training: 100 -> 1000 steps (synth_fb_4) -> pb: NaN loss in reward fun


14/11
- Linear reward with PyTorch (locally) 
	trained agent: 14_11_trained_agent_reward_learning_local.pickle 
	-> with SGD, pb -> one coeff changes far more than the others, loss barely going down (unstable training)
	learning rate: 0.08
- Linear reward remote (Adam) 
	trained agent -> training interrupted
	learning rate: 0.001, now 1000 epsisodes for trainings during reward learning
	Note: problems with reward training -> super slow (cuda?) & loss not going down
	
	
15/11
- Regular agent training with Gabriel's reward (gap2 only) -> local (1000 epi)
	agent: 15_11_trained_agent_gabriel_reward_local.pickle
- Regular agent training with Gabriel's reward (gap2 only) -> remote (10 000 epi)
	agent: 15_11_trained_agent_gabriel_reward_remote.pickle
- Regular agent training with learned reward (gap2 only, 50 rounds) -> local (1000 epi)
	agent: 15_11_trained_agent_learned_reward50_local.pickle
- Regular agent training with learned reward (gap2 only, 50 rounds) -> remote (10 000 epi)
	agent: 15_11_trained_agent_learned_reward50_remote.pickle --> missing!
	

19/11
- Test: automatically set trained_agent name + prin init_reward and reward during training
	lr0.001, Adam
	log: synth_fb_1
	Note: action/closer/sides/opp_sides shoot up, failure goes down (normal), success stationary (normal)
- Test: sort out success rate
	log: synth_fb_2
	
	
20/11
- Test: remote with cuda for reward training
	log: synth_fb_remote_1
- Test: wandb training -> nb of episodes match?
	wandb: 20_11_test_nb_episodes
	no log
- RLHF remote with l2-normalized reward, Adam
	log: synth_fb_remote_2
	trained agent: 20_11_trained_agent_reward_learning_remote_2.pickle
	learning rate: 0.0003, 1000 epsisodes for trainings during reward learning
	

21/11
- Regular agent training with learned reward (gap2 only, 24 rounds) -> remote (10 000 epi)
	agent: 21_11_trained_agent_learned_reward50_remote.pickle  
	wandb: 21_11_trained_agent_learned_reward50_remote
- Reward comparisons: 21_11_reward_comparisons (comparison of 21_11_learned_reward50 and 15_11_gabriel_reward_remote)


26/11
- Test with max pref_dataset size = 50, 200 comparisons => synthetic_feedback_local_1

CORRECTED GABRIEL'S REWARD (cf. thesis)	+ changed hyperparam model, reduced lr to 1e-4
- Regular agent training with Gabriel's reward (gap2-5) -> remote (50k epi)
	agent: 26_11_trained_agent_gabriel_reward_remote_1.pickle -> WAS OVERWRITTEN
	wandb: 26_11_trained_agent_gabriel_reward_remote_1
	Note: for training revert to Gabriel's computation of success_rate
	
- RLHF synthetic fb, 300 comparisons, pref dataset max size 75, 30 rounds
	log: 26_11_synth_fb_remote_1
	agent: 26_11_trained_agent_learned_reward_learning_remote_1.pickle (50k epi)
	wandb: 26_11_synthetic_feedback_remote_1
	
- RLHF synthetic fb, 300 comparisons, pref dataset max size 25 (CHANGE -> this way only look at latest pref), 30 rounds
	log: 26_11_synth_fb_remote_2
	agent: 26_11_trained_agent_learned_reward_learning_remote_2.pickle (50k epi) -> FOR SOME REASON NOT PRESENT...
	wandb: 26_11_synthetic_feedback_remote_2
	
- Regular agent training with Gabriel's reward (gap2-5) -> remote (100k epi)
	agent: 26_11_trained_agent_gabriel_reward_remote.pickle  
	wandb: 26_11_trained_agent_gabriel_reward_remote_2


27/11
TARGET ENTROPY NOW 0.5
- Regular agent training with Gabriel's reward (gap2-5) -> remote (20k epi)
	agent: 27_11_trained_agent_gabriel_reward_remote_1.pt
	wandb: 27_11_trained_agent_gabriel_reward_remote_1
	
- RLHF synthetic fb, 200 comparisons, pref dataset max size 25, 30 rounds, 5000 iter
	log: 27_11_synth_fb_remote_1
	agent: 27_11_trained_agent_learned_reward_learning_remote_1.pt (20k epi)
	wandb: 27_11_synthetic_feedback_remote_1
	
- RLHF synthetic fb, 200 comparisons, pref dataset max size 25, 30 rounds, DISAGREEMENT
	log: 27_11_synth_fb_remote_2
	agent: 27_11_trained_agent_learned_reward_learning_remote_2.pt (20k epi)
	wandb: 27_11_synthetic_feedback_remote_2
	
	
