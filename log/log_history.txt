RLHF corrected (trained reward applied during agent training)

31/10 (gamma = 1 used for reward model): 
- two trainings locally with synthetic fb
- one training on jupyter with synthetic fb -> pb: far too few reward training episodes (forgot to change)

- agents: one trained with Gabriel fb, one with rlhf_local2 (episodes: 10 000) -> TO CHECK!!


07/11 (gamma reward model corrected, evaluation with greedy):
- Added loss tracking for agent training (synth_fb_1) -> weird loss for first few steps (int)
- Weird loss corrected (synth_fb_2)
- Success rate during agent training implemented (synth_fb_3)
- Reward training: 100 -> 1000 steps (synth_fb_4) -> pb: NaN loss in reward fun


14/11
- Linear reward with PyTorch (locally) 
	trained agent: 14_11_trained_agent_reward_learning_local.pickle 
	-> with SGD, pb -> one coeff changes far more than the others, loss barely going down (unstable training)
	learning rate: 0.08
- Linear reward remote (Adam) 
	trained agent -> training interrupted
	learning rate: 0.001, now 1000 epsisodes for trainings during reward learning
	Note: problems with reward training -> super slow (cuda?) & loss not going down
	
	
15/11
- Regular agent training with Gabriel's reward (gap2 only) -> local (1000 epi)
	agent: 15_11_trained_agent_gabriel_reward_local.pickle
- Regular agent training with Gabriel's reward (gap2 only) -> remote (10 000 epi)
	agent: 15_11_trained_agent_gabriel_reward_remote.pickle
- Regular agent training with learned reward (gap2 only, 50 rounds) -> local (1000 epi)
	agent: 15_11_trained_agent_learned_reward50_local.pickle
- Regular agent training with learned reward (gap2 only, 50 rounds) -> remote (10 000 epi)
	agent: 15_11_trained_agent_learned_reward50_remote.pickle --> missing!
	

19/11
- Test: automatically set trained_agent name + prin init_reward and reward during training
	lr0.001, Adam
	log: synth_fb_1
	Note: action/closer/sides/opp_sides shoot up, failure goes down (normal), success stationary (normal)
- Test: sort out success rate
	log: synth_fb_2
	
	
20/11
- Test: remote with cuda for reward training
	log: synth_fb_remote_1
- Test: wandb training -> nb of episodes match?
	wandb: 20_11_test_nb_episodes
	no log
- RLHF remote with l2-normalized reward, Adam
	log: synth_fb_remote_2
	trained agent: 20_11_trained_agent_reward_learning_remote_2.pickle
	learning rate: 0.0003, 1000 epsisodes for trainings during reward learning
	

21/11
- Regular agent training with learned reward (gap2 only, 24 rounds) -> remote (10 000 epi)
	agent: 21_11_trained_agent_learned_reward50_remote.pickle  
	wandb: 21_11_trained_agent_learned_reward50_remote
- Reward comparisons: 21_11_reward_comparisons (comparison of 21_11_learned_reward50 and 15_11_gabriel_reward_remote)


26/11 - PROBLEM: trained RLHF agent for only 1000 epi at each round
- Test with max pref_dataset size = 50, 200 comparisons => synthetic_feedback_local_1

CORRECTED GABRIEL'S REWARD (cf. thesis)	+ changed hyperparam model, reduced lr to 1e-4
- Regular agent training with Gabriel's reward (gap2-5) -> remote (50k epi)
	agent: 26_11_trained_agent_gabriel_reward_remote_1.pickle -> WAS OVERWRITTEN
	wandb: 26_11_trained_agent_gabriel_reward_remote_1
	Note: for training revert to Gabriel's computation of success_rate
	
- RLHF synthetic fb, 300 comparisons, pref dataset max size 75, 30 rounds
	log: 26_11_synth_fb_remote_1
	agent: 26_11_trained_agent_learned_reward_learning_remote_1.pickle (50k epi)
	wandb: 26_11_synthetic_feedback_remote_1
	
- RLHF synthetic fb, 300 comparisons, pref dataset max size 25 (CHANGE -> this way only look at latest pref), 30 rounds
	log: 26_11_synth_fb_remote_2
	agent: 26_11_trained_agent_learned_reward_learning_remote_2.pickle (50k epi) -> FOR SOME REASON NOT PRESENT...
	wandb: 26_11_synthetic_feedback_remote_2
	
- Regular agent training with Gabriel's reward (gap2-5) -> remote (100k epi)
	agent: 26_11_trained_agent_gabriel_reward_remote.pickle  
	wandb: 26_11_trained_agent_gabriel_reward_remote_2


27/11 - PROBLEM: trained RLHF agent for only 1000 epi at each round
TARGET ENTROPY NOW 0.5
- Regular agent training with Gabriel's reward (gap2-5) -> remote (20k epi)
	agent: 27_11_trained_agent_gabriel_reward_remote.pt
	wandb: 27_11_trained_agent_gabriel_reward_remote
	
- RLHF synthetic fb, 200 comparisons, pref dataset max size 25, 30 rounds, 5000 iter
	log: 27_11_synth_fb_remote_1
	agent: 27_11_trained_agent_learned_reward_learning_remote_1.pt (20k epi)
	wandb: 27_11_synthetic_feedback_remote_1
	
- RLHF synthetic fb, 200 comparisons, pref dataset max size 25, 30 rounds, DISAGREEMENT
	log: 27_11_synth_fb_remote_2
	agent: 27_11_trained_agent_learned_reward_learning_remote_2.pt (50k epi) -> MISSING
	wandb: 27_11_synthetic_feedback_remote_2
	
	
01/12 -> interrupted prematurealy cause i'm an idiot
- Regular agent training with Gabriel's reward (gap1-11) -> remote (70k epi)
	agent: 01_12_trained_agent_gabriel_reward_remote.pt
	wandb: 01_12_trained_agent_gabriel_reward_remote
	
	
02/12
- Regular agent training with Gabriel's reward (gap1-11) -> remote (70k epi)
	agent: 02_12_trained_agent_gabriel_reward_remote.pt
	wandb: 02_12_trained_agent_gabriel_reward_remote	
	
- RLHF synthetic fb, 600 comparisons, pref dataset max size 50, 30 rounds, 5000 iter
	log: 02_12_synth_fb_remote_1
	agent: 02_12_trained_agent_learned_reward_learning_remote_1.pt (70k epi)
	wandb: 02_12_synthetic_feedback_remote_1
	
- RLHF synthetic fb, 600 comparisons, pref dataset max size 50, 30 rounds, DISAGREEMENT
	log: 02_12_synth_fb_remote_2
	agent: 02_12_trained_agent_learned_reward_learning_remote_2.pt (70k epi)
	
	
06/12
- Regular agent training with Gabriel's reward (gap1-7) -> remote (40k epi)
	agent: 06_12_trained_agent_gabriel_reward_remote.pt
	wandb: 06_12_trained_agent_gabriel_reward_remote	
	
- RLHF synthetic fb, 400 comparisons, pref dataset max size 100, batch32, 20 rounds, 5000 iter, LINEAR
	log: 06_12_synth_fb_remote_1
	agent: 06_12_trained_agent_learned_reward_learning_remote_1.pt (40k epi)
	wandb: 06_12_synthetic_feedback_remote_1
	
- RLHF synthetic fb, 400 comparisons, pref dataset max size 100, batch32 20 rounds, 5000 iter, LINEAR + DISAGREEMENT
	log: 06_12_synth_fb_remote_2
	agent: 06_12_trained_agent_learned_reward_learning_remote_2.pt (40k epi)
	wandb: 06_12_synthetic_feedback_remote_1
	
	
07/12 -> lower target entropy (0.5 instead of 1.8)
- Regular agent training with Gabriel's reward (gap1-7) -> remote (40k epi)
	agent: 07_12_trained_agent_gabriel_reward_remote.pt CRASHED
	wandb: 07_12_trained_agent_gabriel_reward_remote	
	
- RLHF synthetic fb, 400 comparisons, pref dataset max size 100, batch32, 20 rounds, 5000 iter, LINEAR
	log: 07_12_synth_fb_remote_1
	agent: 07_12_trained_agent_learned_reward_learning_remote_1.pt (40k epi)
	wandb: 07_12_synthetic_feedback_remote_1
	
- RLHF synthetic fb, 400 comparisons, pref dataset max size 100, batch32 20 rounds, 5000 iter, LINEAR + DISAGREEMENT
	log: 07_12_synth_fb_remote_2
	agent: 07_12_trained_agent_learned_reward_learning_remote_2.pt (40k epi) -> NOT THERE
	wandb: 07_12_synthetic_feedback_remote_1

	
10/12
- Regular agent training a few episodes: simple test -> successful


11/12
- Regular agent training with Gabriel's reward (gap1-7) -> remote (20k epi)
	agent: 11_12_trained_agent_gabriel_reward_remote.pt
	wandb: 11_12_trained_agent_gabriel_reward_remote
	
- RLHF synthetic fb, 400 comparisons, pref dataset max size 100, batch32, 20 rounds, 5000 iter, LINEAR
	log: 07_12_synth_fb_remote_1
	agent: 11_12_trained_agent_learned_reward_learning_remote_1.pt (20k epi)
	wandb: 11_12_synthetic_feedback_remote_1
	
- RLHF synthetic fb, 400 comparisons, pref dataset max size 100, batch32 20 rounds, 5000 iter, LINEAR + DISAGREEMENT
	log: 07_12_synth_fb_remote_2
	agent: 11_12_trained_agent_learned_reward_learning_remote_2.pt (20k epi)
	
	
13/12
- RLHF synthetic fb, 40 comparisons, pref dataset max size 100, batch32, 4 rounds, 500 iter, CNN -> JUST A TEST
	log: 13_12_synth_fb_remote_1
	agent: 13_12_trained_agent_learned_reward_learning_remote_1.pt
	wandb: 13_12_synthetic_feedback_remote_1
	wandb: 13_12_synthetic_feedback_remote_1
	
	
16/12
- RLHF CNN tests, lr = 1e-3 - 3e-5 - 1e-5, print average loss over batch + correct loss printing


17/12 -> print/wandb reward_gab & loss
- RLHF synthetic fb, 400 comparisons, pref dataset max size 100, batch32, 20 rounds, 5000 iter, LINEAR
	log: 17_12_synth_fb_remote_1
	agent: 17_12_trained_agent_learned_reward_learning_remote_1.pt (20k epi)
	wandb: 17_12_synthetic_feedback_remote_1
	
- RLHF synthetic fb, 400 comparisons, pref dataset max size 100, batch32, 20 rounds, 5000 iter, CNN
	log: 17_12_synth_fb_remote_2
	agent: 17_12_trained_agent_learned_reward_learning_remote_2.pt (20k epi)
	wandb: 17_12_synthetic_feedback_remote_2


	
