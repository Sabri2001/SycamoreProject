Query schedule: [20, 51, 41, 34, 29, 25]
Collecting 40 trajectories
Trajectory generation started
episode 0/40 rewards: [-1.  0.]
episode 10/40 rewards: [-0.7 -0.1]
episode 20/40 rewards: [-1.  0.]
episode 30/40 rewards: [-1.  0.]
Trajectory generation finished
Creating fragment pairs
Fragmentation done
Gathering preferences
Gathering over
Preferences gathered:  [0.5 0.5 0.5 0.5 1.  0.5 0.  0.  1.  0.5 1.  0.  0.  0.5 0.5 0.  0.  1.
 0.5 0.5]
Dataset now contains 20 comparisons
Training reward model
Epoch [1/40]                     Loss: 0.6911
Epoch [11/40]                     Loss: 0.5234
Epoch [21/40]                     Loss: 0.4460
Epoch [31/40]                     Loss: 0.4119
Reward training finished
Training agent
Training started
episode 0/100 rewards: [-0.7 -0.1]
Success rate:  [1. 0. 0.]
episode 10/100 rewards: [-1.  0.]
Success rate:  [1. 0. 0.]
episode 20/100 rewards: [ 0.3 -1. ]
Success rate:  [1. 0. 0.]
episode 30/100 rewards: [-1.  0.]
Success rate:  [1. 0. 0.]
episode 40/100 rewards: [-1.  0.]
Success rate:  [1. 0. 0.]
episode 50/100 rewards: [-1.  0.]
Success rate:  [1. 0. 0.]
episode 60/100 rewards: [-1.  0.]
Success rate:  [1.     0.     0.0099]
episode 70/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.00895338]
episode 80/100 rewards: [-0.7 -0.1]
Success rate:  [1.         0.         0.00809728]
episode 90/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.00732303]
Training finished
Collecting 102 trajectories
Trajectory generation started
episode 0/102 rewards: [ 0.3 -1. ]
episode 10/102 rewards: [ 0.3 -1. ]
episode 20/102 rewards: [-1.  0.]
episode 30/102 rewards: [ 0.3 -1. ]
episode 40/102 rewards: [-1.  0.]
episode 50/102 rewards: [-1.  0.]
episode 60/102 rewards: [0.2 1.8]
episode 70/102 rewards: [ 0.3 -1. ]
episode 80/102 rewards: [ 0.3 -1. ]
episode 90/102 rewards: [-1.  0.]
episode 100/102 rewards: [-1.  0.]
Trajectory generation finished
Creating fragment pairs
Fragmentation done
Gathering preferences
Gathering over
Preferences gathered:  [0.  1.  1.  0.5 1.  0.  0.5 0.  0.5 0.5 1.  0.5 1.  0.  0.  0.5 0.  1.
 1.  1.  1.  0.  0.  0.  1.  0.5 1.  0.  0.  0.5 0.  0.5 1.  0.5 0.5 0.5
 1.  0.5 1.  0.  0.5 0.5 1.  0.5 0.  0.5 0.5 0.5 0.  0.5 0.5]
Dataset now contains 71 comparisons
Training reward model
Epoch [1/10]                     Loss: 0.3495
Reward training finished
Training agent
Training started
episode 0/100 rewards: [-0.3 -0.1]
Success rate:  [1. 0. 0.]
episode 10/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.01893044]
episode 20/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.02634779]
episode 30/100 rewards: [ 0.2 -0.7]
Success rate:  [1.         0.         0.02382847]
episode 40/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.03145004]
episode 50/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.02844286]
episode 60/100 rewards: [1.7 0.3]
Success rate:  [1.         0.         0.04552421]
episode 70/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.06067527]
episode 80/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.05487363]
episode 90/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.05913662]
Training finished
Collecting 82 trajectories
Trajectory generation started
episode 0/82 rewards: [-1.  0.]
episode 10/82 rewards: [ 0.3 -0.7]
episode 20/82 rewards: [ 0.2 -1.1]
episode 30/82 rewards: [1.7 0.3]
episode 40/82 rewards: [ 0.3 -1. ]
episode 50/82 rewards: [-1.  0.]
episode 60/82 rewards: [ 0.3 -1. ]
episode 70/82 rewards: [-1.  0.]
episode 80/82 rewards: [1.7 0.3]
Trajectory generation finished
Creating fragment pairs
Fragmentation done
Gathering preferences
Gathering over
Preferences gathered:  [1.  0.  1.  0.  0.5 1.  0.5 0.  1.  0.5 1.  1.  0.5 0.5 1.  1.  0.5 0.
 0.5 0.  0.5 0.5 0.  0.5 0.5 0.5 0.5 1.  0.  0.  1.  0.  1.  0.5 0.5 1.
 0.  0.  0.  0.5 0.5]
Dataset now contains 112 comparisons
Training reward model
Epoch [1/10]                     Loss: 0.3187
Reward training finished
Training agent
Training started
episode 0/100 rewards: [ 0.2 -1.1]
Success rate:  [1. 0. 0.]
episode 10/100 rewards: [ 0.2 -1.1]
Success rate:  [1. 0. 0.]
episode 20/100 rewards: [-1.  0.]
Success rate:  [1. 0. 0.]
episode 30/100 rewards: [-0.7 -0.1]
Success rate:  [1. 0. 0.]
episode 40/100 rewards: [ 0.3 -1. ]
Success rate:  [1. 0. 0.]
episode 50/100 rewards: [-1.  0.]
Success rate:  [1. 0. 0.]
episode 60/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.01873545]
episode 70/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.02635881]
episode 80/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.03334834]
episode 90/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.03015964]
Training finished
Collecting 68 trajectories
Trajectory generation started
episode 0/68 rewards: [-1.  0.]
episode 10/68 rewards: [1.7 0.3]
episode 20/68 rewards: [ 0.3 -1. ]
episode 30/68 rewards: [-1.  0.]
episode 40/68 rewards: [-1.  0.]
episode 50/68 rewards: [ 0.3 -1. ]
episode 60/68 rewards: [ 0.3 -1. ]
Trajectory generation finished
Creating fragment pairs
Fragmentation done
Gathering preferences
Gathering over
Preferences gathered:  [1.  0.  1.  0.  0.  1.  0.  1.  0.5 0.  0.  0.5 0.  0.5 0.5 1.  0.  1.
 0.5 1.  0.  0.  1.  0.5 1.  1.  0.  0.  1.  0.  1.  0.  1.  1. ]
Dataset now contains 146 comparisons
Training reward model
Epoch [1/10]                     Loss: 0.2803
Reward training finished
Training agent
Training started
episode 0/100 rewards: [-0.7 -0.1]
Success rate:  [1. 0. 0.]
episode 10/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.00913517]
episode 20/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.01758234]
episode 30/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.02570215]
episode 40/100 rewards: [-0.7 -0.1]
Success rate:  [1.         0.         0.02324457]
episode 50/100 rewards: [-1.  0.]
Success rate:  [1.        0.        0.0495641]
episode 60/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.04482489]
episode 70/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.04053882]
episode 80/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.03666258]
episode 90/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.04276294]
Training finished
Collecting 58 trajectories
Trajectory generation started
episode 0/58 rewards: [ 0.3 -0.7]
episode 10/58 rewards: [ 0.2 -0.7]
episode 20/58 rewards: [-1.  0.]
episode 30/58 rewards: [ 0.2 -1.1]
episode 40/58 rewards: [-1.  0.]
episode 50/58 rewards: [ 0.3 -1. ]
Trajectory generation finished
Creating fragment pairs
Fragmentation done
Gathering preferences
Gathering over
Preferences gathered:  [0.5 0.5 0.5 1.  0.  1.  1.  1.  0.5 0.5 0.  0.  0.5 1.  0.  0.  0.5 0.
 0.  0.5 0.  0.5 0.  0.  0.  0.  1.  0.5 0.5]
Dataset now contains 175 comparisons
Training reward model
Epoch [1/10]                     Loss: 0.2761
Reward training finished
Training agent
Training started
episode 0/100 rewards: [ 0.3 -1. ]
Success rate:  [1. 0. 0.]
episode 10/100 rewards: [ 0.3 -1. ]
Success rate:  [1.        0.        0.0189247]
episode 20/100 rewards: [ 0.2 -0.7]
Success rate:  [1.         0.         0.02652996]
episode 30/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.02399322]
episode 40/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.02169904]
episode 50/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.02885167]
episode 60/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.04531359]
episode 70/100 rewards: [-1.  0.]
Success rate:  [1.        0.        0.0409808]
episode 80/100 rewards: [-1.  0.]
Success rate:  [1.        0.        0.0370623]
episode 90/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.04312444]
Training finished
Collecting 50 trajectories
Trajectory generation started
episode 0/50 rewards: [-1.  0.]
episode 10/50 rewards: [-1.  0.]
episode 20/50 rewards: [-1.  0.]
episode 30/50 rewards: [-1.  0.]
episode 40/50 rewards: [-0.7 -0.1]
Trajectory generation finished
Creating fragment pairs
Fragmentation done
Gathering preferences
Gathering over
Preferences gathered:  [0.5 0.  0.5 1.  0.5 0.5 0.  0.  0.5 0.5 0.  0.  1.  0.5 1.  0.  0.  0.5
 0.5 1.  0.  0.5 1.  0.5 0. ]
Dataset now contains 200 comparisons
Training reward model
Epoch [1/10]                     Loss: 0.2762
Reward training finished
Training agent
Training started
episode 0/100 rewards: [ 0.3 -1. ]
Success rate:  [1. 0. 0.]
episode 10/100 rewards: [-0.7 -0.1]
Success rate:  [1. 0. 0.]
episode 20/100 rewards: [ 0.3 -0.7]
Success rate:  [1.         0.         0.00970299]
episode 30/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.00877521]
episode 40/100 rewards: [1.7 0.3]
Success rate:  [1.         0.         0.01793614]
episode 50/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.01622113]
episode 60/100 rewards: [-1.  0.]
Success rate:  [1.        0.        0.0146701]
episode 70/100 rewards: [-1.  0.]
Success rate:  [1.         0.         0.01326737]
episode 80/100 rewards: [ 0.3 -1. ]
Success rate:  [1.         0.         0.03140867]
episode 90/100 rewards: [0.2 1.8]
Success rate:  [1.         0.         0.03840544]
Training finished
